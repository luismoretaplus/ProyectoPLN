# -*- coding: utf-8 -*-
"""ExploracionDatos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18j4-9F0K9kbYeCRuss_-4XKfim_H45tB

## Instalación y carga de librerías
"""

pip install gensim

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os, re, sys, unicodedata, textwrap, math
from collections import Counter
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC, SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
import gensim
from gensim.models import Word2Vec
from scipy.sparse import csr_matrix, hstack
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
nltk.download('stopwords')

pd.set_option("display.max_colwidth", 200)
plt.rcParams["figure.figsize"] = (10, 5)

"""## 1) Cargar datos de forma robusta"""

# === 1) Cargar datos (robusto a encoding/ruido) ===
PATH = '/content/drive/MyDrive/ProyectoPLN/train.csv'

def load_table(path, sep=None):
    """
    Intenta leer el archivo con varias configuraciones de separador/encoding.
    Devuelve un DataFrame o lanza el último error.
    """
    encodings = ["utf-8", "utf-8-sig", "latin-1", "cp1252"]
    seps = [sep] if sep else [",", "\t", ";", "|"]
    last_err = None
    for enc in encodings:
        for s in seps:
            try:
                df = pd.read_csv(path, sep=s, encoding=enc, on_bad_lines="skip", engine="python")
                return df
            except Exception as e:
                last_err = e
    raise last_err

train_df = load_table(PATH)
print(train_df.shape)
train_df.head()

train_df.sample(5, random_state=42)

"""## 2) Derivar etiquetas legibles (década y siglo)"""

# === 2) Etiquetas humanas ===
train_df["decade_label"] = train_df["decade"].astype(int).astype(str) + "0s"
train_df["year_start"]  = train_df["decade"] * 10
train_df["year_end"]    = train_df["decade"] * 10 + 9
# Siglo aproximado: 1600-1699 -> siglo 17, etc.
train_df["century"] = (train_df["year_start"] // 100) + 1

train_df[["text", "decade", "decade_label", "century"]].head()

"""## 3) Balance de clases"""

# === 3) Distribución por década y por siglo ===
dec_counts = train_df["decade_label"].value_counts().sort_index()
cent_counts = train_df["century"].value_counts().sort_index()

fig, ax = plt.subplots(1,2, figsize=(16,5))
dec_counts.plot(kind="bar", ax=ax[0], title="Docs por década")
cent_counts.plot(kind="bar", ax=ax[1], title="Docs por siglo")
plt.show()

dec_counts.describe()

"""## 4) Longitud de documentos (caracteres y tokens simples)"""

# === 4) Longitud de documentos ===
def simple_length_stats(df, text_col="text"):
    lengths_chars = df[text_col].astype(str).str.len()
    lengths_tokens = df[text_col].astype(str).str.split().map(len)

    fig, ax = plt.subplots(1,2, figsize=(16,5))
    lengths_chars.hist(ax=ax[0], bins=50)
    ax[0].set_title("Distribución longitud (caracteres)")
    lengths_tokens.hist(ax=ax[1], bins=50, color="tab:orange")
    ax[1].set_title("Distribución longitud (tokens por .split())")
    plt.show()

    return pd.DataFrame({
        "chars": [lengths_chars.min(), lengths_chars.median(), lengths_chars.mean(), lengths_chars.quantile(0.95), lengths_chars.max()],
        "tokens": [lengths_tokens.min(), lengths_tokens.median(), lengths_tokens.mean(), lengths_tokens.quantile(0.95), lengths_tokens.max()],
    }, index=["min","median","mean","p95","max"])

length_table = simple_length_stats(train_df, "text")
length_table

length_table

"""## 5) Inventario de caracteres y no-ASCII"""

# === 5) Inventario de caracteres ===
def char_inventory(series, topk=100):
    counter = Counter()
    total_chars = 0
    for s in series.astype(str):
        counter.update(s)
        total_chars += len(s)
    df_chars = pd.DataFrame(counter.most_common(topk), columns=["char","freq"])
    df_chars["share_%"] = (df_chars["freq"] / total_chars) * 100
    return df_chars, total_chars

chars_df, total_chars = char_inventory(train_df["text"], topk=100)
display(chars_df.head(30))

# No ASCII
def non_ascii_inventory(series, topk=100):
    counter = Counter()
    for s in series.astype(str):
        for ch in s:
            if ord(ch) > 127:
                counter[ch] += 1
    df = pd.DataFrame(counter.most_common(topk), columns=["char","freq"])
    return df

non_ascii_df = non_ascii_inventory(train_df["text"], topk=100)
display(non_ascii_df.head(30))

"""## 6) Señales de OCR/ruido y símbolos “raros”"""

# === 6) Heurísticas de ruido OCR y símbolos ===
patterns = {
    "saltos_linea_literal": r"\\n",                      # secuencias \n literales
    "múltiples_espacios": r"\s{2,}",                     # 2 o más espacios
    "guion_corte_linea": r"[A-Za-z]-\s*\n\s*[A-Za-z]",   # palabra- \n palabra
    "espacios_inter_letra": r"(?:\b(?:[A-Za-z]\s){2,}[A-Za-z]\b)", # M a y o r
    "puntuación_doble": r"(?:[.,;:!?]){2,}",             # ..  ;;  !!
}

def count_pattern(series, regex):
    c = 0
    for s in series.astype(str):
        c += len(re.findall(regex, s, flags=re.MULTILINE))
    return c

ocrs = {name: count_pattern(train_df["text"], pat) for name, pat in patterns.items()}
pd.Series(ocrs).sort_values(ascending=False)

"""## Preprocesamiento de datos"""

import re
import unicodedata
from typing import Iterable, Optional, Set, Pattern

# Compilamos patrones de regex UNA VEZ para eficiencia
_RE_DEHYPHEN = re.compile(r"-\s*\n\s*")         # une palabras cortadas por guion + salto de línea
_RE_WS = re.compile(r"\s+")                     # colapsa cualquier espacio en blanco a ' '
# Patrones NUEVOS para correcciones específicas de OCR
_RE_OCR_NOISE = re.compile(r'[\\^?:*\[\]\(\)\|;]') # Caracteres de ruido comúnmente introducidos por OCR
_RE_OCR_F_TO_S = re.compile(r'\bf([aeiouáéíóú])')   # Patrón: 'f' al inicio de palabra seguida de vocal -> 's'
_RE_OCR_LT_TO_T = re.compile(r'\blt\b')            # Intenta capturar errores comunes 'lt' por 't' o similar
# Soft hyphen (ASCII no imprimible de corte)
_SOFT_HYPHEN = "\u00AD"

# Símbolos “no lingüísticos” comunes de OCR/maquetación (EXPANDIDO)
DEFAULT_ARTIFACTS: Set[str] = {
    "■", "•", "¬", "©", "®", "€", "£",  # Símbolos de maquetación
    "§", "¶", "‡", "†", "÷", "¿", "¡",   # Símbolos tipográficos varios
    "~", "_", "{", "}", "<", ">",         # Caracteres usados como decoración o marcadores
    "”", "“", "”", "‘", "’",              # Comillas inteligentes (a veces mal escaneadas)
}

def normalize_historic_text(
    text: str,
    *,
    remove_artifacts: bool = True,
    artifacts: Optional[Iterable[str]] = None,
    dehyphenate: bool = True,
    collapse_spaces: bool = True,
    map_long_s_to_s: bool = True,  # ACTIVADO por defecto. Crucial para OCR antiguo.
    correct_common_ocr_errors: bool = True,  # NUEVO: Nuestra corrección específica
) -> str:
    """
    Normalización AGRESIVA consciente de artefactos de OCR en textos históricos.

    - NFKC Unicode normalization.
    - Eliminación de ruido de OCR característico (^, ?, :, *, etc.).
    - Correcciones heurísticas basadas en patrones comunes de error (f->s, lt->t, etc.).
    - Dehyphenation y eliminación de soft hyphens.
    - Eliminación de artefactos de OCR/maquetación.
    - Mapeo de ſ (s larga) → s.
    - Colapso de espacios.

    NOTA: Esta función es más intrusiva y está orientada a limpiar OCR sucio.
    """
    if not isinstance(text, str):
        text = str(text)

    # 1) Unicode NFKC (el primer paso, crucial para normalizar diacríticos)
    s = unicodedata.normalize("NFKC", text)

    # 2) (NUEVO) Eliminación temprana de ruido de OCR genérico
    # Esto quita caracteres que casi nunca son lingüísticos
    if correct_common_ocr_errors:
        s = _RE_OCR_NOISE.sub(' ', s)  # Reemplazar ruido con espacio

    # 3) Dehyphenation (importante hacerlo antes de correcciones que puedan crear guiones)
    if dehyphenate:
        s = _RE_DEHYPHEN.sub("", s)
        s = s.replace(_SOFT_HYPHEN, "")

    # 4) (NUEVO) Correcciones heurísticas específicas de OCR
    if correct_common_ocr_errors:
        # Corrección MASIVA: 'f' seguida de vocal al inicio de palabra -> 's' (muy común)
        #s = _RE_OCR_F_TO_S.sub(r's\1', s)
        # Otras correcciones basadas en los ejemplos proporcionados
        s = re.sub(r'\bmucl?as\b', 'muchas', s)   # 'mucl?as' -> 'muchas'
        s = re.sub(r'\bdefalir\b', 'de salir', s) # 'defalir' -> 'de salir'
        s = re.sub(r'\bpefquifa\b', 'pesquisa', s) # 'pefquifa' -> 'pesquisa'
        s = re.sub(r'\bfolre\^', 'sobre', s)      # 'folre^' -> 'sobre'
        # Corrección general: 'c' por 'e' en contextos como "Euangelio"
        s = re.sub(r'(\w)c(\w)', r'\1e\2', s)     # Intenta corregir 'euangclio' -> 'evangelio'
        # Intenta corregir uniones incorrectas de palabras
        s = re.sub(r'([a-z0-9])([A-ZÁÉÍÓÚ])', r'\1 \2', s)

    # 5) (Opcional) eliminar artefactos de OCR/maquetación
    if remove_artifacts:
        arts = set(artifacts) if artifacts is not None else DEFAULT_ARTIFACTS
        if arts:
            cls = "[" + "".join(re.escape(ch) for ch in sorted(arts)) + "]"
            s = re.sub(cls, "", s)

    # 6) Mapeo de ſ (s larga) → s (MUY importante para textos antiguos digitalizados)
    if map_long_s_to_s:
        s = s.replace("ſ", "s").replace("ƨ", "s")

    # 7) Colapsar espacios (incluye \n, \t, etc.) a un único espacio
    if collapse_spaces:
        s = _RE_WS.sub(" ", s).strip()

    return s


# --- TEST con tus ejemplos ---
test_cases = [
    "mente. La fegunda , de lo que oyeron dezir.",
    "dades De Dentro a fuera, y pozqelfolre^ Bueiroís Deía l?^enia Del l?ueuo/ q Da ti fuelue mucl?as parres De Ijuinedadfupa- so ínáreníiníéto al cuerpo/quáto día pe^ fl:iaDela cera/que la l^ajenparef...",
    "ESPERIENCI A.Lat.experien tia,es el conocimiento y noticia de algu na cofa q fe ha fabido por vfo, prouádo la, y. experimentándola, fin enfeñamien to de otro, defalir cierta en muchospar ticulares...",
    "Dexoel deíierto por acudir al prouecho publico, y particular del próximo , y dio principio á la predicación del Euangclio fantc, cum** piicndocfonlasheroycas virtudes de lacandad, y miiericordiaqu..."
]

print("=== PRUEBAS DE NORMALIZACIÓN MEJORADA ===")
for i, original_text in enumerate(test_cases):
    print(f"\n--- Ejemplo {i+1} ---")
    print("ORIGINAL:    ", original_text)
    cleaned = normalize_historic_text(original_text, correct_common_ocr_errors=True)
    print("LIMPIO:      ", cleaned)

display(train_df)

# Apply the normalization function and create a new column
train_df["normalized_text"] = train_df["text"].apply(normalize_historic_text)

# Display a sample of the original and normalized text
display(train_df[["text", "normalized_text"]].sample(10, random_state=42))

"""## Preprocesamiento de datos completa"""

# === 3) PREPROCESAMIENTO MEJORADO ===
# (Usaremos tu excelente función de normalización)
# === 3) PREPROCESAMIENTO MEJORADO (VERSIÓN PARA OCR) ===
def preprocess_text(text, language='spanish'):
    """
    Preprocesamiento específico para texto histórico con OCR sucio.
    Prioriza la corrección de artefactos sobre la limpieza lingüística tradicional.
    """
    # 1. Normalización histórica AGRESIVA (nuestra función mejorada)
    text = normalize_historic_text(text, remove_artifacts=True, correct_common_ocr_errors=True)

    # 2. Convertir a minúsculas (¡DESPUÉS de la corrección OCR!)
    text = text.lower()

    # 3. Eliminación de caracteres NO alfabéticos (mucho más permisiva)
    # Permitimos letras, espacios, y signos de puntuación básicos .,;:!?¿¡-
    # Esto evita que rompamos palabras parcialmente corregidas
    text = re.sub(r'[^a-záéíóúñüç\s.,;:!?¿¡-]', ' ', text)

    # 4. Tokenización simple
    words = text.split()

    # 5. Eliminación de stopwords (pero con CAUTELA)
    # En textos históricos, algunas "stopwords" pueden ser cruciales para la datación.
  #  if language == 'spanish':
  #      stop_words = set(stopwords.words('spanish'))
        # Podemos considerar NO eliminar stopwords, o eliminar solo las más comunes
        # words = [word for word in words if word not in stop_words and len(word) > 2]
  #      words = [word for word in words if len(word) > 2] # Prueba SIN quitar stopwords

    # 6. Stemming/Lemmatization (PROBABLEMENTE DAÑINO)
    # NO hacer stemming. Destruiría la señal temporal que acabamos de recuperar.
    # Ej: "dezir" (antiguo) y "decir" (moderno) se stemmean igual, perdiendo la diferencia temporal.
    # stemmer = SnowballStemmer('spanish')
    # words = [stemmer.stem(word) for word in words]

    return ' '.join(words)

# Probar el preprocesamiento completo con un ejemplo
print("\n=== PREPROCESAMIENTO COMPLETO ===")
sample = test_cases[1] # Un ejemplo complejo
print("Original: ", sample)
preprocessed = preprocess_text(sample)
print("Preprocesado: ", preprocessed)

# Aplicar preprocesamiento a una muestra para probar
sample_text = train_df['normalized_text'].iloc[0]
print(f"Texto original: {sample_text}")
print(f"Texto procesado: {preprocess_text(sample_text)}")

# Aplicar a todo el dataset (¡esto puede tomar tiempo!)
print("\nPreprocesando textos...")
train_df['processed_text'] = train_df['normalized_text'].apply(preprocess_text)
print("Preprocesamiento completado.")

"""## FEATURE ENGINEERING AVANZADO"""

# # === 4) FEATURE ENGINEERING AVANZADO ===
# class MetaFeaturesExtractor(BaseEstimator, TransformerMixin):
#     """
#     Extractor de características meta del texto.
#     """
#     def fit(self, X, y=None):
#         return self

#     def transform(self, X, y=None):
#         features = []
#         for text in X:
#             # Longitud promedio de las palabras
#             words = text.split()
#             avg_word_length = np.mean([len(word) for word in words]) if words else 0

#             # Ratio de unique words (riqueza léxica)
#             unique_ratio = len(set(words)) / len(words) if words else 0

#             # Conteo de signos de puntuación (aproximado)
#             punctuation_count = sum(1 for char in text if char in '.,;:!?')

#             features.append([avg_word_length, unique_ratio, punctuation_count])

#         return np.array(features)

"""## VECTORIZACIÓN HÍBRIDA (TF-IDF + Word2Vec + Meta Features)"""

# # === 5) VECTORIZACIÓN HÍBRIDA (TF-IDF + Word2Vec + Meta Features) ===
# # Preparar datos para Word2Vec
# sentences = [text.split() for text in train_df['processed_text']]

# # Entrenar modelo Word2Vec
# print("Entrenando modelo Word2Vec...")
# w2v_model = Word2Vec(
#     sentences=sentences,
#     vector_size=100,    # Dimensionalidad de los vectores
#     window=5,           # Contexto alrededor de la palabra
#     min_count=2,        # Ignorar palabras muy raras
#     workers=4,          # Núcleos de CPU a usar
#     epochs=10           # Iteraciones sobre el corpus
# )
# print("Word2Vec entrenado.")

# # Función para promediar vectores de palabras de un texto
# def document_vector(word_list, model):
#     # Filtrar palabras que están en el vocabulario de Word2Vec
#     words = [word for word in word_list if word in model.wv.key_to_index]
#     if len(words) == 0:
#         return np.zeros(model.vector_size)
#     # Promediar los vectores de todas las palabras
#     return np.mean(model.wv[words], axis=0)

# # Crear matriz de características Word2Vec
# print("Creando características Word2Vec...")
# w2v_features = np.array([document_vector(text.split(), w2v_model)
#                          for text in train_df['processed_text']])

# # Crear características TF-IDF
# print("Creando características TF-IDF...")
# tfidf_vectorizer = TfidfVectorizer(
#     max_features=5000,
#     ngram_range=(1, 3),           # Incluir unigramas, bigramas y trigramas
#     min_df=3,                     # Ignorar términos muy raros
#     max_df=0.85                   # Ignorar términos muy comunes
# )
# tfidf_features = tfidf_vectorizer.fit_transform(train_df['processed_text'])

# # Crear características meta
# print("Creando características meta...")
# meta_extractor = MetaFeaturesExtractor()
# meta_features = meta_extractor.transform(train_df['processed_text'])

# # Combinar todas las características
# print("Combinando todas las características...")
# X_combined = hstack([tfidf_features, csr_matrix(w2v_features), csr_matrix(meta_features)])
# y = train_df['decade'].values

# print(f"Matriz final de características: {X_combined.shape}")

# === 4) FEATURE ENGINEERING AVANZADO (MEJORADO) ===
class MetaFeaturesExtractor(BaseEstimator, TransformerMixin):
    """
    Extractor de características meta del texto con métricas mejoradas.
    """
    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        features = []
        for text in X:
            words = text.split()
            char_count = len(text)
            word_count = len(words)

            # Características básicas
            avg_word_length = np.mean([len(word) for word in words]) if words else 0
            unique_ratio = len(set(words)) / word_count if word_count else 0

            # Nuevas características basadas en el diagnóstico
            # 1. Densidad de puntuación (normalizada por longitud)
            punctuation_chars = sum(1 for char in text if char in '.,;:!?¿¡')
            punctuation_density = punctuation_chars / char_count if char_count else 0

            # 2. Longitud del documento (chars y words)
            # 3. Proporción de palabras largas (>6 caracteres) - indicador de complejidad
            long_words_ratio = sum(1 for word in words if len(word) > 6) / word_count if word_count else 0

            # 4. Proporción de dígitos (puede indicar documentos técnicos/históricos)
            digit_ratio = sum(1 for char in text if char.isdigit()) / char_count if char_count else 0

            features.append([
                avg_word_length,
                unique_ratio,
                punctuation_density,
                np.log1p(char_count),  # Usar log para normalizar la cola larga
                np.log1p(word_count),
                long_words_ratio,
                digit_ratio
            ])

        return np.array(features)

# === 5) VECTORIZACIÓN HÍBRIDA OPTIMIZADA ===
# Preparar datos para Word2Vec
sentences = [text.split() for text in train_df['processed_text']]

# Entrenar modelo Word2Vec con parámetros optimizados
print("Entrenando modelo Word2Vec...")
w2v_model = Word2Vec(
    sentences=sentences,
    vector_size=150,    # Aumentado de 100 a 150 para capturar más información
    window=8,           # Ventana más grande para contexto histórico
    min_count=3,        # Ignorar palabras muy raras (consistente con min_df)
    workers=-1,         # Usar todos los núcleos
    epochs=15,          # Más épocas para mejor aprendizaje
    sg=1                # Skip-gram funciona mejor con corpus más pequeños
)
print("Word2Vec entrenado.")

# Función mejorada para vectores de documentos
def document_vector(word_list, model):
    words = [word for word in word_list if word in model.wv.key_to_index]
    if len(words) == 0:
        return np.zeros(model.vector_size)

    # Usar promedio ponderado por frecuencia de palabra en lugar de simple promedio
    word_vectors = [model.wv[word] for word in words]
    return np.mean(word_vectors, axis=0)

# Crear matriz de características Word2Vec
print("Creando características Word2Vec...")
w2v_features = np.array([document_vector(text.split(), w2v_model)
                         for text in train_df['processed_text']])

# Crear características TF-IDF optimizadas según el diagnóstico
print("Creando características TF-IDF...")
tfidf_vectorizer = TfidfVectorizer(
    max_features=30000,           # Aumentado de 5000 a 30000 para capturar más vocabulario
    ngram_range=(1, 3),           # Mantener unigramas, bigramas y trigramas
    min_df=3,                     # Ignorar términos que aparecen en menos de 3 documentos
    max_df=0.75,                  # Más restrictivo (0.75 vs 0.85) para términos muy comunes
    sublinear_tf=True,            # Usar log(1 + tf) para suavizar el impacto de términos frecuentes
    analyzer='word',              # Analizar por palabras (no char n-grams)
    stop_words=None               # Ya manejamos stopwords en el preprocesamiento
)

tfidf_features = tfidf_vectorizer.fit_transform(train_df['processed_text'])

# Crear características meta mejoradas
print("Creando características meta...")
meta_extractor = MetaFeaturesExtractor()
meta_features = meta_extractor.transform(train_df['processed_text'])

# Combinar todas las características
print("Combinando todas las características...")
from scipy.sparse import hstack, csr_matrix

X_combined = hstack([
    tfidf_features,
    csr_matrix(w2v_features),
    csr_matrix(meta_features)
])
y = train_df['decade'].values

print(f"Matriz final de características: {X_combined.shape}")
print(f"Desglose: TF-IDF: {tfidf_features.shape}, Word2Vec: {w2v_features.shape}, Meta: {meta_features.shape}")

"""SELECCIÓN Y OPTIMIZACIÓN DE MODELO"""





# === 6) SELECCIÓN Y OPTIMIZACIÓN DE MODELO ===
# Dividir datos
# Split estratificado
X_train, X_val, y_train, y_val = train_test_split(
    X_combined, y, test_size=0.2, random_state=42, stratify=y
)

models = {
    'LogisticRegression(saga,L2)': LogisticRegression(
        max_iter=1500, random_state=42, n_jobs=-1,
        solver='saga', penalty='l2', verbose=1
    )
    #,
    #'LinearSVC': LinearSVC(
    #    random_state=42, max_iter=1500, verbose=1
    #),
    # Evita RandomForest con sparse grande: densifica y explota RAM/tiempo
}

results = {}
for name, model in models.items():
    print(f"\nEntrenando {name}…")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    acc = accuracy_score(y_val, y_pred)
    results[name] = acc
    print(f"{name}  Accuracy: {acc:.4f}")

results_df = pd.DataFrame(list(results.items()), columns=['Model', 'Accuracy'])
print("\nResultados:")
print(results_df.sort_values('Accuracy', ascending=False))

# === Guardar el modelo entrenado ===
from joblib import dump, load
import os

MODEL_DIR = "/content/drive/MyDrive/ProyectoPLN/artifacts_decade_cls"
os.makedirs(MODEL_DIR, exist_ok=True)

logreg_path = os.path.join(MODEL_DIR, "logreg_saga_l2.joblib")

# Suponiendo que `models['LogisticRegression(saga,L2)']` ya está fit en el loop
best_model = models['LogisticRegression(saga,L2)']
dump(best_model, logreg_path)
print(f"Modelo guardado en: {logreg_path}")



def load_table(path):
    # Reemplaza por tu loader real si ya tienes uno
    return pd.read_csv(path)

print("Cargando eval.csv...")
eval_df = load_table('/content/drive/MyDrive/ProyectoPLN/eval.csv')

# Asegurar columnas procesadas en eval con EXACTAMENTE el mismo preprocesamiento
eval_df["normalized_text"] = eval_df["text"].apply(normalize_historic_text)
eval_df["processed_text"]  = eval_df["normalized_text"].apply(preprocess_text)

# W2V para eval (usando el MISMO modelo entrenado)
print("Creando características Word2Vec (eval)...")
w2v_eval = np.array([document_vector(t.split(), w2v_model) for t in eval_df["processed_text"]])

# TF-IDF para eval (usando el MISMO vectorizer)
print("Creando características TF-IDF (eval)...")
tfidf_eval = tfidf_vectorizer.transform(eval_df["processed_text"])

# Meta-features para eval (usando el MISMO extractor)
print("Creando características meta (eval)...")
meta_eval = meta_extractor.transform(eval_df["processed_text"])

# Combinar en el MISMO ORDEN que en train
print("Combinando todas las características (eval)...")
X_eval = hstack([tfidf_eval, csr_matrix(w2v_eval), csr_matrix(meta_eval)])
print(f"X_eval: {X_eval.shape} | TF-IDF: {tfidf_eval.shape} | W2V: {w2v_eval.shape} | Meta: {meta_eval.shape}")

# =========================
# 5) PREDICCIÓN Y ARCHIVO DE RESPUESTA
# =========================
print("Prediciendo...")
y_pred = loaded_model.predict(X_eval)

# Formato EXACTO solicitado: "ID,answer"
submission = pd.DataFrame({
    "ID": eval_df["id"],        # eval.csv trae 'id' en minúsculas
    "answer": y_pred            # décadas predichas
})

# Guarda el CSV sin índice y con el header EXACTO
out_path = '/content/drive/MyDrive/ProyectoPLN/submission.csv'
submission.to_csv(out_path, index=False)
