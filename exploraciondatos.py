# -*- coding: utf-8 -*-
"""ExploracionDatos.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18j4-9F0K9kbYeCRuss_-4XKfim_H45tB

## Instalación y carga de librerías
"""

# pip install gensim

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os, re, sys, unicodedata, textwrap, math, random
from collections import Counter
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import LinearSVC, SVC
from sklearn.ensemble import RandomForestClassifier, VotingClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import FunctionTransformer, StandardScaler
import gensim
from gensim.models import Word2Vec
from scipy.sparse import csr_matrix
from scipy import sparse
from joblib import dump, load
import nltk
from nltk.corpus import stopwords
from nltk.stem import SnowballStemmer
nltk.download('stopwords')

pd.set_option("display.max_colwidth", 200)
plt.rcParams["figure.figsize"] = (10, 5)

RANDOM_SEED = 42
np.random.seed(RANDOM_SEED)
random.seed(RANDOM_SEED)
os.environ["PYTHONHASHSEED"] = str(RANDOM_SEED)

"""## 1) Cargar datos de forma robusta"""

# === 1) Cargar datos (robusto a encoding/ruido) ===
PATH = '/content/drive/MyDrive/ProyectoPLN/train.csv'

def load_table(path, sep=None):
    """
    Intenta leer el archivo con varias configuraciones de separador/encoding.
    Devuelve un DataFrame o lanza el último error.
    """
    encodings = ["utf-8", "utf-8-sig", "latin-1", "cp1252"]
    seps = [sep] if sep else [",", "\t", ";", "|"]
    last_err = None
    for enc in encodings:
        for s in seps:
            try:
                df = pd.read_csv(path, sep=s, encoding=enc, on_bad_lines="skip", engine="python")
                return df
            except Exception as e:
                last_err = e
    raise last_err

train_df = load_table(PATH)
print(train_df.shape)
train_df.head()

train_df.sample(5, random_state=42)

"""## 2) Derivar etiquetas legibles (década y siglo)"""

# === 2) Etiquetas humanas ===
train_df["decade_label"] = train_df["decade"].astype(int).astype(str) + "0s"
train_df["year_start"]  = train_df["decade"] * 10
train_df["year_end"]    = train_df["decade"] * 10 + 9
# Siglo aproximado: 1600-1699 -> siglo 17, etc.
train_df["century"] = (train_df["year_start"] // 100) + 1

train_df[["text", "decade", "decade_label", "century"]].head()

"""## 3) Balance de clases"""

# === 3) Distribución por década y por siglo ===
dec_counts = train_df["decade_label"].value_counts().sort_index()
cent_counts = train_df["century"].value_counts().sort_index()

fig, ax = plt.subplots(1,2, figsize=(16,5))
dec_counts.plot(kind="bar", ax=ax[0], title="Docs por década")
cent_counts.plot(kind="bar", ax=ax[1], title="Docs por siglo")
plt.show()

dec_counts.describe()

"""## 4) Longitud de documentos (caracteres y tokens simples)"""

# === 4) Longitud de documentos ===
def simple_length_stats(df, text_col="text"):
    lengths_chars = df[text_col].astype(str).str.len()
    lengths_tokens = df[text_col].astype(str).str.split().map(len)

    fig, ax = plt.subplots(1,2, figsize=(16,5))
    lengths_chars.hist(ax=ax[0], bins=50)
    ax[0].set_title("Distribución longitud (caracteres)")
    lengths_tokens.hist(ax=ax[1], bins=50, color="tab:orange")
    ax[1].set_title("Distribución longitud (tokens por .split())")
    plt.show()

    return pd.DataFrame({
        "chars": [lengths_chars.min(), lengths_chars.median(), lengths_chars.mean(), lengths_chars.quantile(0.95), lengths_chars.max()],
        "tokens": [lengths_tokens.min(), lengths_tokens.median(), lengths_tokens.mean(), lengths_tokens.quantile(0.95), lengths_tokens.max()],
    }, index=["min","median","mean","p95","max"])

length_table = simple_length_stats(train_df, "text")
length_table

length_table

"""## 5) Inventario de caracteres y no-ASCII"""

# === 5) Inventario de caracteres ===
def char_inventory(series, topk=100):
    counter = Counter()
    total_chars = 0
    for s in series.astype(str):
        counter.update(s)
        total_chars += len(s)
    df_chars = pd.DataFrame(counter.most_common(topk), columns=["char","freq"])
    df_chars["share_%"] = (df_chars["freq"] / total_chars) * 100
    return df_chars, total_chars

chars_df, total_chars = char_inventory(train_df["text"], topk=100)
display(chars_df.head(30))

# No ASCII
def non_ascii_inventory(series, topk=100):
    counter = Counter()
    for s in series.astype(str):
        for ch in s:
            if ord(ch) > 127:
                counter[ch] += 1
    df = pd.DataFrame(counter.most_common(topk), columns=["char","freq"])
    return df

non_ascii_df = non_ascii_inventory(train_df["text"], topk=100)
display(non_ascii_df.head(30))

"""## 6) Señales de OCR/ruido y símbolos “raros”"""

# === 6) Heurísticas de ruido OCR y símbolos ===
patterns = {
    "saltos_linea_literal": r"\\n",                      # secuencias \n literales
    "múltiples_espacios": r"\s{2,}",                     # 2 o más espacios
    "guion_corte_linea": r"[A-Za-z]-\s*\n\s*[A-Za-z]",   # palabra- \n palabra
    "espacios_inter_letra": r"(?:\b(?:[A-Za-z]\s){2,}[A-Za-z]\b)", # M a y o r
    "puntuación_doble": r"(?:[.,;:!?]){2,}",             # ..  ;;  !!
}

def count_pattern(series, regex):
    c = 0
    for s in series.astype(str):
        c += len(re.findall(regex, s, flags=re.MULTILINE))
    return c

ocrs = {name: count_pattern(train_df["text"], pat) for name, pat in patterns.items()}
pd.Series(ocrs).sort_values(ascending=False)

"""## Preprocesamiento de datos"""

import re
import unicodedata
from typing import Iterable, Optional, Set, Pattern

# Compilamos patrones de regex UNA VEZ para eficiencia
_RE_DEHYPHEN = re.compile(r"-\s*\n\s*")         # une palabras cortadas por guion + salto de línea
_RE_WS = re.compile(r"\s+")                     # colapsa cualquier espacio en blanco a ' '
# Patrones NUEVOS para correcciones específicas de OCR
_RE_OCR_NOISE = re.compile(r'[\\^?:*\[\]\(\)\|;]') # Caracteres de ruido comúnmente introducidos por OCR
_RE_OCR_F_TO_S = re.compile(r'\bf([aeiouáéíóú])')   # Patrón: 'f' al inicio de palabra seguida de vocal -> 's'
_RE_OCR_LT_TO_T = re.compile(r'\blt\b')            # Intenta capturar errores comunes 'lt' por 't' o similar
# Soft hyphen (ASCII no imprimible de corte)
_SOFT_HYPHEN = "\u00AD"

# Símbolos “no lingüísticos” comunes de OCR/maquetación (EXPANDIDO)
DEFAULT_ARTIFACTS: Set[str] = {
    "■", "•", "¬", "©", "®", "€", "£",  # Símbolos de maquetación
    "§", "¶", "‡", "†", "÷", "¿", "¡",   # Símbolos tipográficos varios
    "~", "_", "{", "}", "<", ">",         # Caracteres usados como decoración o marcadores
    "”", "“", "”", "‘", "’",              # Comillas inteligentes (a veces mal escaneadas)
}

def normalize_historic_text(
    text: str,
    *,
    remove_artifacts: bool = True,
    artifacts: Optional[Iterable[str]] = None,
    dehyphenate: bool = True,
    collapse_spaces: bool = True,
    map_long_s_to_s: bool = True,  # ACTIVADO por defecto. Crucial para OCR antiguo.
    correct_common_ocr_errors: bool = True,  # NUEVO: Nuestra corrección específica
) -> str:
    """
    Normalización AGRESIVA consciente de artefactos de OCR en textos históricos.

    - NFKC Unicode normalization.
    - Eliminación de ruido de OCR característico (^, ?, :, *, etc.).
    - Correcciones heurísticas basadas en patrones comunes de error (f->s, lt->t, etc.).
    - Dehyphenation y eliminación de soft hyphens.
    - Eliminación de artefactos de OCR/maquetación.
    - Mapeo de ſ (s larga) → s.
    - Colapso de espacios.

    NOTA: Esta función es más intrusiva y está orientada a limpiar OCR sucio.
    """
    if not isinstance(text, str):
        text = str(text)

    # 1) Unicode NFKC (el primer paso, crucial para normalizar diacríticos)
    s = unicodedata.normalize("NFKC", text)

    # 2) (NUEVO) Eliminación temprana de ruido de OCR genérico
    # Esto quita caracteres que casi nunca son lingüísticos
    if correct_common_ocr_errors:
        s = _RE_OCR_NOISE.sub(' ', s)  # Reemplazar ruido con espacio

    # 3) Dehyphenation (importante hacerlo antes de correcciones que puedan crear guiones)
    if dehyphenate:
        s = _RE_DEHYPHEN.sub("", s)
        s = s.replace(_SOFT_HYPHEN, "")

    # 4) (NUEVO) Correcciones heurísticas específicas de OCR
    if correct_common_ocr_errors:
        # Corrección MASIVA: 'f' seguida de vocal al inicio de palabra -> 's' (muy común)
        #s = _RE_OCR_F_TO_S.sub(r's\1', s)
        # Otras correcciones basadas en los ejemplos proporcionados
        s = re.sub(r'\bmucl?as\b', 'muchas', s)   # 'mucl?as' -> 'muchas'
        s = re.sub(r'\bdefalir\b', 'de salir', s) # 'defalir' -> 'de salir'
        s = re.sub(r'\bpefquifa\b', 'pesquisa', s) # 'pefquifa' -> 'pesquisa'
        s = re.sub(r'\bfolre\^', 'sobre', s)      # 'folre^' -> 'sobre'
        # Corrección general: 'c' por 'e' en contextos como "Euangelio"
        s = re.sub(r'(\w)c(\w)', r'\1e\2', s)     # Intenta corregir 'euangclio' -> 'evangelio'
        # Intenta corregir uniones incorrectas de palabras
        s = re.sub(r'([a-z0-9])([A-ZÁÉÍÓÚ])', r'\1 \2', s)

    # 5) (Opcional) eliminar artefactos de OCR/maquetación
    if remove_artifacts:
        arts = set(artifacts) if artifacts is not None else DEFAULT_ARTIFACTS
        if arts:
            cls = "[" + "".join(re.escape(ch) for ch in sorted(arts)) + "]"
            s = re.sub(cls, "", s)

    # 6) Mapeo de ſ (s larga) → s (MUY importante para textos antiguos digitalizados)
    if map_long_s_to_s:
        s = s.replace("ſ", "s").replace("ƨ", "s")

    # 7) Colapsar espacios (incluye \n, \t, etc.) a un único espacio
    if collapse_spaces:
        s = _RE_WS.sub(" ", s).strip()

    return s


# --- TEST con tus ejemplos ---
test_cases = [
    "mente. La fegunda , de lo que oyeron dezir.",
    "dades De Dentro a fuera, y pozqelfolre^ Bueiroís Deía l?^enia Del l?ueuo/ q Da ti fuelue mucl?as parres De Ijuinedadfupa- so ínáreníiníéto al cuerpo/quáto día pe^ fl:iaDela cera/que la l^ajenparef...",
    "ESPERIENCI A.Lat.experien tia,es el conocimiento y noticia de algu na cofa q fe ha fabido por vfo, prouádo la, y. experimentándola, fin enfeñamien to de otro, defalir cierta en muchospar ticulares...",
    "Dexoel deíierto por acudir al prouecho publico, y particular del próximo , y dio principio á la predicación del Euangclio fantc, cum** piicndocfonlasheroycas virtudes de lacandad, y miiericordiaqu..."
]

print("=== PRUEBAS DE NORMALIZACIÓN MEJORADA ===")
for i, original_text in enumerate(test_cases):
    print(f"\n--- Ejemplo {i+1} ---")
    print("ORIGINAL:    ", original_text)
    cleaned = normalize_historic_text(original_text, correct_common_ocr_errors=True)
    print("LIMPIO:      ", cleaned)

display(train_df)

# Apply the normalization function and create a new column
train_df["normalized_text"] = train_df["text"].apply(normalize_historic_text)

# Display a sample of the original and normalized text
display(train_df[["text", "normalized_text"]].sample(10, random_state=42))

"""## Preprocesamiento de datos completa"""

# === 3) PREPROCESAMIENTO MEJORADO ===
# (Usaremos tu excelente función de normalización)
# === 3) PREPROCESAMIENTO MEJORADO (VERSIÓN PARA OCR) ===
def preprocess_text(text, language='spanish'):
    """
    Preprocesamiento específico para texto histórico con OCR sucio.
    Prioriza la corrección de artefactos sobre la limpieza lingüística tradicional.
    """
    # 1. Normalización histórica AGRESIVA (nuestra función mejorada)
    text = normalize_historic_text(text, remove_artifacts=True, correct_common_ocr_errors=True)

    # 2. Convertir a minúsculas (¡DESPUÉS de la corrección OCR!)
    text = text.lower()

    # 3. Eliminación de caracteres NO alfabéticos (mucho más permisiva)
    # Permitimos letras, espacios, y signos de puntuación básicos .,;:!?¿¡-
    # Esto evita que rompamos palabras parcialmente corregidas
    text = re.sub(r'[^a-záéíóúñüç\s.,;:!?¿¡-]', ' ', text)

    # 4. Tokenización simple
    words = text.split()

    # 5. Eliminación de stopwords (pero con CAUTELA)
    # En textos históricos, algunas "stopwords" pueden ser cruciales para la datación.
  #  if language == 'spanish':
  #      stop_words = set(stopwords.words('spanish'))
        # Podemos considerar NO eliminar stopwords, o eliminar solo las más comunes
        # words = [word for word in words if word not in stop_words and len(word) > 2]
  #      words = [word for word in words if len(word) > 2] # Prueba SIN quitar stopwords

    # 6. Stemming/Lemmatization (PROBABLEMENTE DAÑINO)
    # NO hacer stemming. Destruiría la señal temporal que acabamos de recuperar.
    # Ej: "dezir" (antiguo) y "decir" (moderno) se stemmean igual, perdiendo la diferencia temporal.
    # stemmer = SnowballStemmer('spanish')
    # words = [stemmer.stem(word) for word in words]

    return ' '.join(words)

# Probar el preprocesamiento completo con un ejemplo
print("\n=== PREPROCESAMIENTO COMPLETO ===")
sample = test_cases[1] # Un ejemplo complejo
print("Original: ", sample)
preprocessed = preprocess_text(sample)
print("Preprocesado: ", preprocessed)

# Aplicar preprocesamiento a una muestra para probar
sample_text = train_df['normalized_text'].iloc[0]
print(f"Texto original: {sample_text}")
print(f"Texto procesado: {preprocess_text(sample_text)}")

# Aplicar a todo el dataset (¡esto puede tomar tiempo!)
print("\nPreprocesando textos...")
train_df['processed_text'] = train_df['normalized_text'].apply(preprocess_text)
print("Preprocesamiento completado.")

"""## FEATURE ENGINEERING AVANZADO"""

# # === 4) FEATURE ENGINEERING AVANZADO ===
# class MetaFeaturesExtractor(BaseEstimator, TransformerMixin):
#     """
#     Extractor de características meta del texto.
#     """
#     def fit(self, X, y=None):
#         return self

#     def transform(self, X, y=None):
#         features = []
#         for text in X:
#             # Longitud promedio de las palabras
#             words = text.split()
#             avg_word_length = np.mean([len(word) for word in words]) if words else 0

#             # Ratio de unique words (riqueza léxica)
#             unique_ratio = len(set(words)) / len(words) if words else 0

#             # Conteo de signos de puntuación (aproximado)
#             punctuation_count = sum(1 for char in text if char in '.,;:!?')

#             features.append([avg_word_length, unique_ratio, punctuation_count])

#         return np.array(features)

"""## VECTORIZACIÓN HÍBRIDA (TF-IDF + Word2Vec + Meta Features)"""


class MetaFeaturesExtractor(BaseEstimator, TransformerMixin):
    """Extractor de características meta del texto con métricas mejoradas."""

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        features = []
        for text in X:
            words = text.split()
            char_count = len(text)
            word_count = len(words)

            avg_word_length = np.mean([len(word) for word in words]) if words else 0
            unique_ratio = len(set(words)) / word_count if word_count else 0
            punctuation_chars = sum(1 for char in text if char in '.,;:!?¿¡')
            punctuation_density = punctuation_chars / char_count if char_count else 0
            long_words_ratio = sum(1 for word in words if len(word) > 6) / word_count if word_count else 0
            digit_ratio = sum(1 for char in text if char.isdigit()) / char_count if char_count else 0

            features.append([
                avg_word_length,
                unique_ratio,
                punctuation_density,
                np.log1p(char_count),
                np.log1p(word_count),
                long_words_ratio,
                digit_ratio,
            ])

        return np.array(features)


def to_csr_matrix(X):
    """Asegura salida CSR para compatibilidad con combinaciones dispersas."""

    if sparse.issparse(X):
        return X
    if isinstance(X, pd.Series):
        X = X.values
    if isinstance(X, list):
        X = np.asarray(X)
    return csr_matrix(X)


class TextPreprocessor(BaseEstimator, TransformerMixin):
    """Normaliza y limpia texto histórico dentro del pipeline."""

    def fit(self, X, y=None):
        return self

    def transform(self, X, y=None):
        processed = [preprocess_text(text) for text in X]
        return np.array(processed, dtype=object)


class Word2VecTfidfVectorizer(BaseEstimator, TransformerMixin):
    """Genera vectores de documento ponderados por TF-IDF a partir de Word2Vec."""

    def __init__(
        self,
        vector_size=150,
        window=8,
        min_count=3,
        epochs=15,
        workers=None,
        seed=42,
        max_features=20000,
    ):
        self.vector_size = vector_size
        self.window = window
        self.min_count = min_count
        self.epochs = epochs
        self.workers = workers
        self.seed = seed
        self.max_features = max_features

    def fit(self, X, y=None):
        self.sentences_ = [doc.split() for doc in X]
        workers = self.workers if self.workers is not None else (os.cpu_count() or 1)
        self.model_ = Word2Vec(
            sentences=self.sentences_,
            vector_size=self.vector_size,
            window=self.window,
            min_count=self.min_count,
            workers=workers,
            epochs=self.epochs,
            seed=self.seed,
        )
        self.word_tfidf_vectorizer_ = TfidfVectorizer(
            analyzer="word",
            ngram_range=(1, 1),
            max_features=self.max_features,
        )
        self.word_tfidf_vectorizer_.fit(X)
        self.feature_names_ = np.array(
            self.word_tfidf_vectorizer_.get_feature_names_out()
        )
        return self

    def transform(self, X, y=None):
        if not hasattr(self, "model_"):
            raise RuntimeError("Word2VecTfidfVectorizer debe ajustarse antes de transform().")

        transformed = []
        for doc in X:
            tokens = doc.split()
            if not tokens:
                transformed.append(np.zeros(self.vector_size, dtype=np.float64))
                continue

            tfidf_vec = self.word_tfidf_vectorizer_.transform([doc])
            if tfidf_vec.nnz == 0:
                transformed.append(np.zeros(self.vector_size, dtype=np.float64))
                continue

            weights = {
                self.feature_names_[idx]: value
                for idx, value in zip(tfidf_vec.indices, tfidf_vec.data)
            }

            weighted_sum = np.zeros(self.vector_size, dtype=np.float64)
            weight_total = 0.0
            for token in tokens:
                if token in weights and token in self.model_.wv.key_to_index:
                    weight = weights[token]
                    weighted_sum += self.model_.wv[token] * weight
                    weight_total += weight

            if weight_total == 0.0:
                transformed.append(np.zeros(self.vector_size, dtype=np.float64))
            else:
                transformed.append(weighted_sum / weight_total)

        if not transformed:
            return np.zeros((0, self.vector_size), dtype=np.float64)

        return np.vstack(transformed)


print("\n=== CONFIGURANDO PIPELINE HÍBRIDA ===")

tfidf_branch = TfidfVectorizer(
    max_features=20000,
    ngram_range=(1, 2),
    min_df=3,
    max_df=0.75,
    sublinear_tf=True,
    analyzer="word",
)

w2v_branch = Pipeline(
    steps=[
        (
            "w2v",
            Word2VecTfidfVectorizer(
                vector_size=150,
                window=8,
                min_count=3,
                epochs=15,
                workers=None,
                seed=RANDOM_SEED,
                max_features=20000,
            ),
        ),
        ("to_sparse", FunctionTransformer(to_csr_matrix, accept_sparse=True)),
    ]
)

meta_branch = Pipeline(
    steps=[
        ("meta", MetaFeaturesExtractor()),
        ("scaler", StandardScaler(with_mean=False)),
        ("to_sparse", FunctionTransformer(to_csr_matrix, accept_sparse=True)),
    ]
)

hybrid_features = FeatureUnion(
    transformer_list=[
        ("tfidf", tfidf_branch),
        ("w2v", w2v_branch),
        ("meta", meta_branch),
    ]
)

text_pipeline = Pipeline(
    steps=[
        ("preprocess", TextPreprocessor()),
        ("features", hybrid_features),
    ]
)

preprocessor = ColumnTransformer(
    transformers=[("text", text_pipeline, "text")],
    remainder="drop",
)

classifier = LogisticRegression(
    solver="saga",
    penalty="elasticnet",
    l1_ratio=0.5,
    C=0.5,
    max_iter=2000,
    class_weight="balanced",
    n_jobs=-1,
    random_state=RANDOM_SEED,
)

hybrid_pipeline = Pipeline(
    steps=[
        ("preprocessor", preprocessor),
        ("classifier", classifier),
    ]
)

X = train_df[["text"]]
y = train_df["decade"].astype(int).values

X_train, X_val, y_train, y_val = train_test_split(
    X,
    y,
    test_size=0.2,
    stratify=y,
    random_state=RANDOM_SEED,
)

print("Entrenando pipeline híbrida sobre el conjunto de entrenamiento...")
hybrid_pipeline.fit(X_train, y_train)

val_predictions = hybrid_pipeline.predict(X_val)
val_accuracy = accuracy_score(y_val, val_predictions)
val_f1 = f1_score(y_val, val_predictions, average="macro")

print(f"Accuracy de validación: {val_accuracy:.4f}")
print(f"F1 macro de validación: {val_f1:.4f}")

print("Entrenando pipeline final sobre el dataset completo...")
hybrid_pipeline.fit(X, y)

MODEL_DIR = "/content/drive/MyDrive/ProyectoPLN/artifacts_decade_cls"
os.makedirs(MODEL_DIR, exist_ok=True)

ARTIFACT_PATH = os.path.join(MODEL_DIR, "artifacts.joblib")

text_transformer = hybrid_pipeline.named_steps["preprocessor"].named_transformers_["text"]
features_union = text_transformer.named_steps["features"]
tfidf_vectorizer = dict(features_union.transformer_list)["tfidf"]
w2v_pipeline = dict(features_union.transformer_list)["w2v"]
meta_pipeline = dict(features_union.transformer_list)["meta"]

artifacts = {
    "pipeline": hybrid_pipeline,
    "tfidf": tfidf_vectorizer,
    "w2v": w2v_pipeline.named_steps["w2v"].model_,
    "meta_scaler": meta_pipeline.named_steps["scaler"],
}

dump(artifacts, ARTIFACT_PATH)
print(f"Artefactos del modelo guardados en: {ARTIFACT_PATH}")

print("Cargando eval.csv...")
eval_df = load_table('/content/drive/MyDrive/ProyectoPLN/eval.csv')

loaded_artifacts = load(ARTIFACT_PATH)
loaded_pipeline = loaded_artifacts["pipeline"]

print("Generando predicciones sobre el conjunto de evaluación...")
eval_predictions = loaded_pipeline.predict(eval_df[["text"]])

submission = pd.DataFrame({
    "ID": eval_df["id"],
    "answer": eval_predictions,
})

out_path = '/content/drive/MyDrive/ProyectoPLN/submission.csv'
submission.to_csv(out_path, index=False)
print(f"Archivo de predicción guardado en: {out_path}")
